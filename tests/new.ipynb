{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2266d831-aaec-41fe-b2c6-ed84e8f5e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, mean,expr, avg, stddev\n",
    "from pyspark.sql.functions import lag, coalesce, lit\n",
    "from pyspark.sql.functions import to_date, date_format\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478d613a-8aa2-4960-a335-f66f05e9ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Streaming Fraud Detection\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1311b70-2e4d-4069-bafc-065e0ace1229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+----------------+--------+--------+-----+------+-------+---------+--------+---+----------+----------+---------+-----------+--------+---+----+---+-----+\n",
      "|_c0|trans_date_trans_time|          cc_num|merchant|category|  amt|gender|    lat|     long|city_pop|job|       dob| unix_time|merch_lat| merch_long|is_fraud|age|hour|day|month|\n",
      "+---+---------------------+----------------+--------+--------+-----+------+-------+---------+--------+---+----------+----------+---------+-----------+--------+---+----+---+-----+\n",
      "|  0|  2020-06-13 07:35:03|6511349151405438|      29|       4|166.8|     1|39.3426|-114.8859|     450|254|1946-08-24|1371108903|40.088507|-113.895268|       0| 79|   7|  5|    6|\n",
      "|  1|  2019-09-12 19:09:06|3566094707272327|     536|      11|28.86|     0|34.3795| -118.523|   34882|219|1971-04-25|1347476946|35.356925|-119.348148|       0| 54|  19|  3|    9|\n",
      "|  2|  2020-02-14 05:31:05|3573030041201292|     153|       2|37.93|     0|40.3207| -110.436|     302|406|1990-01-17|1360819865|40.422976|-110.786285|       0| 35|   5|  4|    2|\n",
      "|  3|  2020-05-08 00:33:15| 213173753804333|     677|       9| 18.7|     1|41.2244| -86.6966|    5791|134|1959-10-07|1367973195|40.254936| -85.751919|       0| 66|   0|  4|    5|\n",
      "|  4|  2020-09-05 02:31:11| 373905417449658|     688|       8|33.54|     0| 31.929| -97.6443|    2526|342|1970-11-12|1378348271|32.397579| -97.395488|       0| 55|   2|  5|    9|\n",
      "+---+---------------------+----------------+--------+--------+-----+------+-------+---------+--------+---+----------+----------+---------+-----------+--------+---+----+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = r\"credit_card_transactions.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "073ddf43-c607-4ee5-890f-8fcd5d938d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- merchant: integer (nullable = true)\n",
      " |-- category: integer (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: integer (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- unix_time: integer (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "Rows: 11, Columns: 20\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "print(f\"Rows: {df.count()}, Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709ff4d7-d78c-490b-b7f8-23906a41e07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|            _c0|              cc_num|          merchant|         category|               amt|            gender|               lat|              long|         city_pop|               job|           unix_time|         merch_lat|        merch_long|           is_fraud|               age|             hour|               day|             month|\n",
      "+-------+---------------+--------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|             11|                  11|                11|               11|                11|                11|                11|                11|               11|                11|                  11|                11|                11|                 11|                11|               11|                11|                11|\n",
      "|   mean|            5.0|3.018688652466434E15| 413.3636363636364|6.181818181818182|128.20818181818183|0.6363636363636364|37.193054545454544|-98.72955454545458|5666.272727272727|240.54545454545453|        1.36606957E9|37.376699272727265|-98.64478145454545|0.09090909090909091| 55.09090909090909|7.909090909090909|4.2727272727272725|6.2727272727272725|\n",
      "| stddev|3.3166247903554|2.072798655615327...|237.19623636443842| 3.18804585344006|203.77708565087397| 0.504524979109513| 4.282715826987273|14.355661819182258|9871.547012408026| 89.95817209832983|1.4072269011230854E7| 4.029370052820567|14.246699138667921|0.30151134457776363|17.598037080620927| 5.87289614167568|1.7939291563999449|2.6491851234260353|\n",
      "|    min|              0|      30408301059761|                29|                2|              5.88|                 0|            31.882|          -118.523|              302|                96|          1335956341|         32.397579|       -119.348148|                  0|                28|                0|                 0|                 2|\n",
      "|    max|             10|    6511349151405438|               688|               11|            713.56|                 1|           44.1111|          -79.7372|            34882|               406|          1385717672|         43.255179|        -80.387158|                  1|                81|               19|                 6|                11|\n",
      "+-------+---------------+--------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52687f25-4602-4274-a465-ee88cbe66c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+------+--------+--------+---+------+---+----+--------+---+---+---------+---------+----------+--------+---+----+---+-----+\n",
      "|_c0|trans_date_trans_time|cc_num|merchant|category|amt|gender|lat|long|city_pop|job|dob|unix_time|merch_lat|merch_long|is_fraud|age|hour|day|month|\n",
      "+---+---------------------+------+--------+--------+---+------+---+----+--------+---+---+---------+---------+----------+--------+---+----+---+-----+\n",
      "|  0|                    0|     0|       0|       0|  0|     0|  0|   0|       0|  0|  0|        0|        0|         0|       0|  0|   0|  0|    0|\n",
      "+---+---------------------+------+--------+--------+---+------+---+----+--------+---+---+---------+---------+----------+--------+---+----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "missing_values.show()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdbb5e61-ad02-44f6-8c69-6dc4f008ce07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+----------------+--------+--------+-----+------+-------+---------+--------+---+----------+----------+---------+-----------+--------+---+----+---+-----+----------+\n",
      "|_c0|trans_date_trans_time|          cc_num|merchant|category|  amt|gender|    lat|     long|city_pop|job|       dob| unix_time|merch_lat| merch_long|is_fraud|age|hour|day|month|trans_date|\n",
      "+---+---------------------+----------------+--------+--------+-----+------+-------+---------+--------+---+----------+----------+---------+-----------+--------+---+----+---+-----+----------+\n",
      "|  0|  2020-06-13 07:35:03|6511349151405438|      29|       4|166.8|     1|39.3426|-114.8859|     450|254|1946-08-24|1371108903|40.088507|-113.895268|       0| 79|   7|  5|    6|2020-06-13|\n",
      "|  1|  2019-09-12 19:09:06|3566094707272327|     536|      11|28.86|     0|34.3795| -118.523|   34882|219|1971-04-25|1347476946|35.356925|-119.348148|       0| 54|  19|  3|    9|2019-09-12|\n",
      "|  2|  2020-02-14 05:31:05|3573030041201292|     153|       2|37.93|     0|40.3207| -110.436|     302|406|1990-01-17|1360819865|40.422976|-110.786285|       0| 35|   5|  4|    2|2020-02-14|\n",
      "|  3|  2020-05-08 00:33:15| 213173753804333|     677|       9| 18.7|     1|41.2244| -86.6966|    5791|134|1959-10-07|1367973195|40.254936| -85.751919|       0| 66|   0|  4|    5|2020-05-08|\n",
      "|  4|  2020-09-05 02:31:11| 373905417449658|     688|       8|33.54|     0| 31.929| -97.6443|    2526|342|1970-11-12|1378348271|32.397579| -97.395488|       0| 55|   2|  5|    9|2020-09-05|\n",
      "+---+---------------------+----------------+--------+--------+-----+------+-------+---------+--------+---+----------+----------+---------+-----------+--------+---+----+---+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Convert to timestamp \n",
    "df = df.withColumn(\"trans_date_trans_time\", col(\"trans_date_trans_time\").cast(\"timestamp\"))\n",
    "\n",
    "# Create a new column containing only the date (while keeping the original column).\n",
    "df = df.withColumn(\"trans_date\", to_date(col(\"trans_date_trans_time\")))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ded8645-9c60-495c-a210-4e5dc1ea19cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set row count: 8\n",
      "Testing set row count: 3\n",
      "+---+---------------------+----------------+--------+--------+-----+------+-------+---------+--------+---+----------+----------+---------+-----------+--------+---+----+---+-----+----------+\n",
      "|_c0|trans_date_trans_time|          cc_num|merchant|category|  amt|gender|    lat|     long|city_pop|job|       dob| unix_time|merch_lat| merch_long|is_fraud|age|hour|day|month|trans_date|\n",
      "+---+---------------------+----------------+--------+--------+-----+------+-------+---------+--------+---+----------+----------+---------+-----------+--------+---+----+---+-----+----------+\n",
      "|  7|  2020-08-23 10:10:29|4147608975828480|     548|       2|46.64|     1|44.1111| -94.9134|     914|218|1944-07-26|1377252629|43.255179| -94.744586|       0| 81|  10|  6|    8|2020-08-23|\n",
      "|  6|  2020-11-29 09:34:32|3560697798177746|     518|       2|75.57|     1|33.7163|-116.3381|    4677|265|1955-05-06|1385717672|34.696349|-115.852896|       0| 70|   9|  6|   11|2020-11-29|\n",
      "|  4|  2020-09-05 02:31:11| 373905417449658|     688|       8|33.54|     0| 31.929| -97.6443|    2526|342|1970-11-12|1378348271|32.397579| -97.395488|       0| 55|   2|  5|    9|2020-09-05|\n",
      "+---+---------------------+----------------+--------+--------+-----+------+-------+---------+--------+---+----------+----------+---------+-----------+--------+---+----+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# order the dates in ascending order\n",
    "df = df.orderBy('trans_date_trans_time')\n",
    "# Calculate split index\n",
    "split_index = int(df.count() * 0.8)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train = df.limit(split_index)  # Take the first 80% of rows\n",
    "test = df.subtract(train)      # Subtract the training set from the original DataFrame to get the test set\n",
    "test_copy = test.select(\"*\") \n",
    "# Display row counts of the resulting DataFrames to verify the split\n",
    "print(f\"Training set row count: {train.count()}\")\n",
    "print(f\"Testing set row count: {test.count()}\")\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e094725-807e-467b-bb4c-e72c0f045f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [ \"merchant\", \"category\", \"amt\", \"gender\", \"lat\", \"long\", \"city_pop\", \"job\", \"unix_time\", \"merch_lat\", \"merch_long\", \"age\", \"hour\", \"day\", \"month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7be32859-768e-4d29-b354-fa5b0d1ee8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data chronologically (80% train, 20% test)\n",
    "total_rows = df.count()\n",
    "train_rows = int(total_rows * 0.8)\n",
    "train_df = df.orderBy(\"trans_date_trans_time\").limit(train_rows)\n",
    "test_df = df.orderBy(\"trans_date_trans_time\").exceptAll(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15a6731a-875d-4e69-8b57-cdda90a5270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set row count: 8\n",
      "Testing set row count: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set row count: {train_df.count()}\")\n",
    "print(f\"Testing set row count: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e3d3e28-5c03-4117-a2ef-3f9eb1270bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+--------+\n",
      "|trans_date_trans_time|     scaled_features|is_fraud|\n",
      "+---------------------+--------------------+--------+\n",
      "|  2019-05-02 10:59:01|[0.90136570561456...|       0|\n",
      "|  2019-09-12 19:09:06|[0.76934749620637...|       0|\n",
      "|  2020-02-14 05:31:05|[0.18816388467374...|       0|\n",
      "|  2020-04-11 12:47:16|[0.09408194233687...|       0|\n",
      "|  2020-05-08 00:33:15|[0.98330804248861...|       0|\n",
      "+---------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "# Convert the timestamp to a Unix timestamp (if not already).\n",
    "df = df.withColumn(\"unix_time\", unix_timestamp(\"trans_date_trans_time\").cast(\"int\"))\n",
    "\n",
    "# Create a data processing pipeline.\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# Train the pipeline on the dataset.\n",
    "transformer = pipeline.fit(df)\n",
    "\n",
    "# Transform the data.\n",
    "df = transformer.transform(df).select(\"trans_date_trans_time\", \"scaled_features\", \"is_fraud\")\n",
    "train_set = transformer.transform(train_df).select(\"trans_date_trans_time\", \"scaled_features\", \"is_fraud\")\n",
    "test_set = transformer.transform(test_df).select(\"trans_date_trans_time\", \"scaled_features\", \"is_fraud\")\n",
    "\n",
    "train_set.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20302123-bd2f-4429-84df-776b83049721",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; background-color: white; font-family: Lobster;color: black; padding: 14px; line-height: 1;border-radius:12px\"> Logistic Regression</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2efddba1-62d5-405f-b591-381bab731827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Logistic Regression Model\n",
    "logistic_regressor = LogisticRegression(\n",
    "    featuresCol='scaled_features', \n",
    "    labelCol='is_fraud'\n",
    ")\n",
    "\n",
    "# Use `BinaryClassificationEvaluator` instead of `RegressionEvaluator`.\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol='is_fraud', \n",
    "    rawPredictionCol='prediction', \n",
    "    metricName='areaUnderROC'  # Có thể đổi thành 'areaUnderPR' nếu cần\n",
    ")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(logistic_regressor.regParam, [0.001, 0.01, 0.1]) \\\n",
    "    .addGrid(logistic_regressor.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(logistic_regressor.maxIter, [50, 100, 300]) \\\n",
    "    .addGrid(logistic_regressor.tol, [1e-6, 1e-4, 1e-2]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "256fb5a5-a31d-4d2b-bf2c-e3bc1ba34cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Params:\n",
      "  Regularization Param (regParam): 0.001\n",
      "  ElasticNet Param (elasticNetParam): 0.0\n",
      "  Maximum Iterations (maxIter): 50\n",
      "  Tolerance (tol): 1e-06\n",
      "  Threshold: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation with numFolds=3\n",
    "crossval = CrossValidator(\n",
    "    estimator=logistic_regressor,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "# Train the best model on the training set.\n",
    "cv_model = crossval.fit(train_set)\n",
    "\n",
    "# Retrieve the best model from Cross Validation.\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "print(\"Best Model Params:\")\n",
    "print(\"  Regularization Param (regParam):\", best_model.getRegParam())\n",
    "print(\"  ElasticNet Param (elasticNetParam):\", best_model.getElasticNetParam())\n",
    "print(\"  Maximum Iterations (maxIter):\", best_model.getMaxIter())\n",
    "print(\"  Tolerance (tol):\", best_model.getTol())\n",
    "print(\"  Threshold:\", best_model.getThreshold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d767fc0f-3fb2-4317-82f2-aac706df3f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.0\n",
      "AUC-PR: 0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Make predictions on the test set.\n",
    "prediction_test = best_model.transform(test_set)\n",
    "\n",
    "# Evaluate the model based on AUC-ROC and AUC-PR.\n",
    "evaluator_roc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_fraud\",\n",
    "    rawPredictionCol=\"prediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_fraud\",\n",
    "    rawPredictionCol=\"prediction\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "# Calculate the evaluation metrics.\n",
    "auc_roc = evaluator_roc.evaluate(prediction_test)\n",
    "auc_pr = evaluator_pr.evaluate(prediction_test)\n",
    "\n",
    "print(\"AUC-ROC:\", auc_roc)\n",
    "print(\"AUC-PR:\", auc_pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79c59bc2-2bab-4632-800f-0570fd883192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+--------+----------+\n",
      "|trans_date_trans_time|     scaled_features|is_fraud|prediction|\n",
      "+---------------------+--------------------+--------+----------+\n",
      "|  2020-08-23 10:10:29|[0.78755690440060...|       0|       0.0|\n",
      "|  2020-11-29 09:34:32|[0.74203338391502...|       0|       0.0|\n",
      "|  2020-09-05 02:31:11|[1.0,0.6666666666...|       0|       0.0|\n",
      "+---------------------+--------------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = test_set.join(prediction_test.select(\"trans_date_trans_time\",\"prediction\"), on=\"trans_date_trans_time\", how=\"left\")\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edb573e1-cd32-4f33-8922-b96d2c42945c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-23 10:10:29</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-29 09:34:32</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-05 02:31:11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trans_date_trans_time  is_fraud  prediction\n",
       "0   2020-08-23 10:10:29         0         0.0\n",
       "1   2020-11-29 09:34:32         0         0.0\n",
       "2   2020-09-05 02:31:11         0         0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predictions.toPandas()\n",
    "predictions = predictions.drop(\"scaled_features\", axis=1)\n",
    "predictions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec4a9614-5f27-4cc0-af63-d4204f0dc44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library new for Random Forest Classifier, Gradient-Boosted Trees (GBT) Classifier, Decision Tree Classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a1f6ca-4740-4b62-a64e-985ff741384f",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; background-color: white; font-family: Lobster;color: black; padding: 14px; line-height: 1;border-radius:12px\"> Random Forest Classifier</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "136e7e77-7ff3-4bf2-9f7f-42b3b08513e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Params:\n",
      "  Num Trees: 50\n",
      "  Max Depth: 5\n",
      "AUC-ROC: 0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    featuresCol='scaled_features', \n",
    "    labelCol='is_fraud'\n",
    ")\n",
    "\n",
    "# Evaluator use AUC-ROC\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol='is_fraud', \n",
    "    rawPredictionCol='prediction', \n",
    "    metricName='areaUnderROC'\n",
    ")\n",
    "\n",
    "# Create grid search for hyperparameters\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf_classifier.numTrees, [50, 100, 200]) \\\n",
    "    .addGrid(rf_classifier.maxDepth, [5, 10, 20]) \\\n",
    "    .addGrid(rf_classifier.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation with numFolds=3\n",
    "crossval = CrossValidator(\n",
    "    estimator=rf_classifier,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "\n",
    "cv_model = crossval.fit(train_set)\n",
    "\n",
    "\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "print(\"Best Model Params:\")\n",
    "print(\"  Num Trees:\", best_model.getNumTrees)\n",
    "print(\"  Max Depth:\", best_model.getMaxDepth())\n",
    "\n",
    "\n",
    "prediction_test = best_model.transform(test_set)\n",
    "\n",
    "\n",
    "auc_roc = evaluator.evaluate(prediction_test)\n",
    "print(\"AUC-ROC:\", auc_roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0daca9f-a2b5-4d09-8452-a97a4c03f8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0faa4d96-107e-4da7-8570-7e7d4b2e97b4",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; background-color: white; font-family: Lobster;color: black; padding: 14px; line-height: 1;border-radius:12px\"> LSTM</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56eeef37-77ef-46eb-93a7-1b3b798e4def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features Shape: (8, 15)\n",
      "Test Features Shape: (3, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.7500 - loss: 0.6903 - val_accuracy: 1.0000 - val_loss: 0.6675\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8750 - loss: 0.6773 - val_accuracy: 1.0000 - val_loss: 0.6517\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8750 - loss: 0.6701 - val_accuracy: 1.0000 - val_loss: 0.6354\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.8750 - loss: 0.6538 - val_accuracy: 1.0000 - val_loss: 0.6181\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8750 - loss: 0.6405 - val_accuracy: 1.0000 - val_loss: 0.5994\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8750 - loss: 0.6335 - val_accuracy: 1.0000 - val_loss: 0.5788\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8750 - loss: 0.6207 - val_accuracy: 1.0000 - val_loss: 0.5560\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8750 - loss: 0.6014 - val_accuracy: 1.0000 - val_loss: 0.5305\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8750 - loss: 0.5882 - val_accuracy: 1.0000 - val_loss: 0.5019\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8750 - loss: 0.5643 - val_accuracy: 1.0000 - val_loss: 0.4696\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 0.4696\n",
      "LSTM Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "# Convert PySpark data to NumPy for use with TensorFlow\n",
    "train_features = np.array(train_set.select(\"scaled_features\").toPandas()[\"scaled_features\"].tolist())\n",
    "train_labels = np.array(train_set.select(\"is_fraud\").toPandas()[\"is_fraud\"])\n",
    "\n",
    "test_features = np.array(test_set.select(\"scaled_features\").toPandas()[\"scaled_features\"].tolist())\n",
    "test_labels = np.array(test_set.select(\"is_fraud\").toPandas()[\"is_fraud\"])\n",
    "\n",
    "# Check the size before reshaping\n",
    "print(\"Train Features Shape:\", train_features.shape)\n",
    "print(\"Test Features Shape:\", test_features.shape)\n",
    "\n",
    "# Reshape data for LSTM\n",
    "train_features = train_features.reshape((train_features.shape[0], train_features.shape[1], 1))\n",
    "test_features = test_features.reshape((test_features.shape[0], test_features.shape[1], 1))\n",
    "\n",
    "# Create LSTM\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(train_features.shape[1], 1)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(train_features, train_labels, epochs=10, batch_size=32, validation_data=(test_features, test_labels))\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "print(\"LSTM Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "173c3b1a-3b02-4eb9-9f51-f0514ed2a309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Logistic Regression Evaluation Metrics:\n",
      "   ✅ AUC-ROC: 0.0000\n",
      "   ✅ AUC-PR: 0.0000\n",
      "   ✅ MSE: 0.0000\n",
      "   ✅ RMSE: 0.0000\n",
      "   ✅ MAE: 0.0000\n",
      "   ✅ R² Score: nan\n",
      "\n",
      "🔹 Random Forest Evaluation Metrics:\n",
      "   ✅ AUC-ROC: 0.0000\n",
      "   ✅ AUC-PR: 0.0000\n",
      "   ✅ MSE: 0.0000\n",
      "   ✅ RMSE: 0.0000\n",
      "   ✅ MAE: 0.0000\n",
      "   ✅ R² Score: nan\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier\n",
    ")\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "\n",
    "# 1️⃣ Logistic Regression\n",
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"is_fraud\")\n",
    "lr_model = lr.fit(train_set)\n",
    "prediction_test = lr_model.transform(test_set)\n",
    "\n",
    "# 2️⃣ Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"is_fraud\", numTrees=50)\n",
    "rf_model = rf.fit(train_set)\n",
    "rf_predictions = rf_model.transform(test_set)\n",
    "\n",
    "# 3️⃣ lSTM\n",
    "\n",
    "\n",
    "# Dictionary containing models and their corresponding predictions\n",
    "models = {\n",
    "    \"Logistic Regression\": prediction_test, \n",
    "    \"Random Forest\": rf_predictions, \n",
    "}\n",
    "\n",
    "# Define the evaluators\n",
    "evaluator_roc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_fraud\",\n",
    "    rawPredictionCol=\"rawPrediction\",  # Sửa lại rawPrediction thay vì prediction\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_fraud\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "evaluator_mse = RegressionEvaluator(\n",
    "    labelCol=\"is_fraud\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mse\"\n",
    ")\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=\"is_fraud\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=\"is_fraud\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=\"is_fraud\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "for model_name, predictions in models.items():\n",
    "    auc_roc = evaluator_roc.evaluate(predictions)\n",
    "    auc_pr = evaluator_pr.evaluate(predictions)\n",
    "    mse = evaluator_mse.evaluate(predictions)\n",
    "    rmse = evaluator_rmse.evaluate(predictions)\n",
    "    mae = evaluator_mae.evaluate(predictions)\n",
    "    r2 = evaluator_r2.evaluate(predictions)\n",
    "    \n",
    "    print(f\"\\n🔹 {model_name} Evaluation Metrics:\")\n",
    "    print(f\"   ✅ AUC-ROC: {auc_roc:.4f}\")\n",
    "    print(f\"   ✅ AUC-PR: {auc_pr:.4f}\")\n",
    "    print(f\"   ✅ MSE: {mse:.4f}\")\n",
    "    print(f\"   ✅ RMSE: {rmse:.4f}\")\n",
    "    print(f\"   ✅ MAE: {mae:.4f}\")\n",
    "    print(f\"   ✅ R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748da4bc-0c03-45b3-a96e-d7ef385b30bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd58408-7c0d-4bb4-bdcb-7d6291877645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
